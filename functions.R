### Functions

# # get the data
# loader = function() {
#   train_df = read_csv(here::here("data/train.csv"))
#   test_df = read_csv(here::here("data/test.csv")) # we are going to drop this later! just including it for clarity
#   df_list = list(train_df, test_df)
#   df_list
# }

# get the data
loader = function() {
  df= read_csv(here::here("data/train.csv"))
  df
}

# counter
counter = function(x) {
  (sum(is.na(x))/length(x))
}

# lumper (to lump small factor levels)
lumper = function(x, perc_lump) {
  fct_lump(x, prop = perc_lump)
}

# combine and format the data
combiner = function(data, list = NULL, perc_miss, perc_lump) {
  x = data %>%
    clean_names() %>% # clean column names
    mutate_if(is.character, as.factor) %>% # mutate character columns to factors
    mutate_at(vars(c(contains("year"), contains("yr"), contains("class"))), as.factor) %>% 
    mutate_at(list, as.factor)
  #rowid_to_column("id") # need to make if_then version of this 
  y = map_df(x, counter) %>% # get variables that have fewer than 20% of their values missing 
    pivot_longer(-id, names_to = "values", values_to = "count") %>%
    select(-id) %>%
    filter(count < perc_miss) %>%
    pull(values)
  z_factors = x %>%
    select(one_of(y)) %>% # filter the df from x to the subset of cols in y
    drop_na() %>% # drop rows containing NAs 
    mutate_if(is.factor, function(u) lumper(u, perc_lump)) %>%  # lump levels of factors together into "other" if they are 10% or fewer of the observed vals
    mutate_if(is.factor, function(x) lumper(x, -0.89)) %>% # lump levels of factors together if any level shows up > 89% of the time
    select_if(is.factor) # drop non-factor variables
  z_factors2 = z_factors[, sapply(z_factors, nlevels) > 1]  # EVERYTHING BETWEEN HERE IS GROSS
  z_factors3 = sapply(z_factors2, fct_drop) %>% 
    as_tibble()
  z_factors4 = z_factors3[, sapply(z_factors3, function(x) length(unique(x))) > 1] %>% 
    as_tibble() %>% 
    mutate_if(is.character, as.factor) %>% 
    rowid_to_column() # EVERYTHING BETWEEN HERE IS GROSS
  z = x %>%
    select(one_of(y)) %>% # filter the df from x to the subset of cols in y
    select_if(is.numeric) %>% 
    drop_na() %>% # drop rows containing NAs 
    rowid_to_column() %>% 
    inner_join(z_factors4, by = "rowid") %>% 
    select(-rowid)
  z
}

tasker = function(data, id, target) { # data: df produced by combiner (dataframe), id: identifier for the new task (string), target: column in df 
  # that corresponds to the target vector of our model (vector)
  task = TaskRegr$new(id = id, backend = data, target = target)
  task
}

learn_gener = function(lrn_list) { # ... : a vector of learners (e.g c("regr.rpart", "regr.ranger"))
  learners = map(lrn_list, lrn, predict_sets = c("train", "test"))
  learners
}

splitter = function(data, prop, seed) { # x: an mlr3 task (TaskRegr), prop: proportion of data for train set (scalar), seed: random seed (scalar)
  set.seed(seed)                        # returns a list of row ids 
  train = sample(data$nrow, prop*data$nrow)
  test = setdiff(seq_len(data$nrow), train)
  list(train, test)
}

trainer = function(learners, task, df_list) { # learners: list of learners generated by learn_gener (list), task: mlr3 task generated by tasker (TaskRegr),
  # df_list: list of dataframes generated by splitter (list)
  trained_learners = lapply(learners, function(x) x$train(task, row_ids = df_list[[1]]))
  trained_learners
}

resampler = function(method, folds, seed) {
  set.seed(seed)
  method = rsmp(method, folds = folds)
  method
}

benchmarker = function(task, learners, resampling, measures) {
  measures_loaded = c(lapply(measures, function(x) msr(x, id = str_c(x, "train", sep = "_"), predict_sets = "train")), 
                      lapply(measures, function(x) msr(x, id = str_c(x, "test", sep = "_"), predict_sets = "test")))
  design = benchmark_grid(task, learners, resampling)
  bmr = benchmark(design)
  bmr_done = bmr$aggregate(measures_loaded)
  bmr_done
}

predictor = function(trained_model, task, test_ids) {
  preds = lapply(trained_model, function(x) x$predict(task, row_ids = test_ids))
  preds
}

fixer = function(task, target) {
  target_free = as_tibble(task$data()) %>% select(-target)
  target_vec = as_tibble(task$data()) %>% select(target) %>% pull()
  list(target_free, target_vec)
}

imler_local = function(trained_learners, lrn_list, data, target, observation){
  preds = lapply(trained_learners, function(x) Predictor$new(x, data = as_tibble(data), 
                                                             y = target))
  limes = lapply(preds, function(x) LocalModel$new(x, x.interest = as_tibble(data) %>% slice(observation), k = 10))
  shaps = lapply(preds, function(x) Shapley$new(x, x.interest = as_tibble(data) %>% slice(observation)))
  lime_plots = lapply(limes, function(x) ggplot(as_tibble(x$results), aes(reorder(x$results$feature.value, x$results$effect), x$results$effect)) +
                        geom_bar(stat = "identity") +
                        coord_flip())
  shap_plots = lapply(shaps, function(x) ggplot(as_tibble(x$results), aes(reorder(x$results$feature.value, x$results$phi), x$results$phi)) +
                        geom_bar(stat = "identity") +
                        coord_flip())
  lime_plots2 = list()
  shap_plots2 = list()
  for (i in 1:length(trained_learners)) {
    lime_plots2[[i]] = lime_plots[[i]] +
      labs(x = "Feature Value", y= "Feature Effect (on prediction)", title = paste("LIME Effects for Model", lrn_list[[i]], "at Observation", observation))
  }
  for (i in 1:length(trained_learners)) {
    shap_plots2[[i]] = shap_plots[[i]] + 
      labs(x = "Feature Values", y = "Phi (Shapley Value)", title = paste("Shapley Values for Model", lrn_list[[i]], "at Observation", observation))
  }
  list(lime_plots2, shap_plots2)
}

imler_permutation = function(trained_learners, lrn_list, data, target) {
  preds = lapply(trained_learners, function(x) Predictor$new(x, data = as_tibble(data), 
                                                             y = target))
  imps = lapply(preds, function(x) FeatureImp$new(x, loss = "mae"))
  smol_imps = lapply(imps, function(x) x$results %>% top_n(20))
  global_plots = lapply(smol_imps, function(x) ggplot(x, aes(importance, reorder(feature, importance))) + 
                       geom_point(size = 3) +
                       geom_errorbarh(aes(xmin = importance.05, xmax = importance.95), size = 0.3))
  imp_plots2 = list()
  for (i in 1:length(trained_learners)) {
    imp_plots2[[i]] = global_plots[[i]] + 
      labs(x = "Importance", y = "Feature", title = paste("Permutation importance for", lrn_list[[i]]))
  }
  imp_plots2
}

imler_global_plots = function(trained_learners, lrn_list, data, target, feature = NULL, ice = NULL, pdp_ice = NULL, pdp = NULL, ale = NULL) {
  preds = lapply(trained_learners, function(x) Predictor$new(x, data = as_tibble(data), 
                                                             y = target))
  if (pdp == TRUE) {
    pdp_data = lapply(preds, function(x) FeatureEffects$new(x, method = "pdp", feature = feature))
    pdp_data_cleaned = list()
    for (i in 1:length(pdp_data)) {
      pdp_data_cleaned[[i]] = pdp_data[[i]]$results %>% 
        data.frame() %>% 
        as_tibble() %>% 
        transmute(feature_val = gr_liv_area..feature,
                  y_hat = gr_liv_area..y.hat,
                  type = gr_liv_area..type)
    }
    pdp_plots = lapply(pdp_data_cleaned, function(x) ggplot(x, aes(feature_val, y_hat)) + 
                         geom_line())
    pdp_plots2 = list()
    for (i in 1:length(trained_learners)) {
      pdp_plots2[[i]] = pdp_plots[[i]] +
        labs(x = "Permuted Feature Value", y= "Y-hat", title = paste("PDP for Model", lrn_list[[i]], "on Feature", feature))
    }
  } else {
    break
  }
  if (pdp_ice == TRUE) {
    pdp_ice_data = lapply(preds, function(x) FeatureEffects$new(x, method = "pdp+ice", feature = feature))
    pdp_ice_data_cleaned = list()
    for (i in 1:length(pdp_ice_data)) { 
      pdp_ice_data_cleaned[[i]] = pdp_ice_data[[i]]$results %>% 
        data.frame() %>% 
        as_tibble() %>% 
        transmute(feature_val = gr_liv_area..feature,
                  y_hat = gr_liv_area..y.hat,
                  type = gr_liv_area..type,
                  id = gr_liv_area..id)
    }
    ice_plots = list()
    for (i in 1:length(pdp_ice_data)) {
      ice_plots[[i]] = ggplot() + 
        geom_line(pdp_ice_data_cleaned[[i]] %>% filter(type == "ice"), mapping = aes(feature_val, y_hat/1000, group = id), alpha = 0.2) +
        labs(x = "Feature Value", y = "Sale Price in Thousands", title = paste("ICE Plot of Sale Price for", lrn_list[[i]], "on Feature", feature))
    }
    pdp_ice_plots = list()
    for (i in 1:length(pdp_ice_data)) {
      pdp_ice_plots[[i]] = ggplot() + 
        geom_line(pdp_ice_data_cleaned[[i]] %>% filter(type == "ice"), mapping = aes(feature_val, y_hat/1000, group = id), alpha = 0.2) +
        geom_line(pdp_ice_data_cleaned[[i]] %>% filter(type == "pdp"), mapping = aes(feature_val, y_hat/1000), color = "red", size = 3) +
        labs(x = "Feature Value", y = "Sale Price in Thousands", title = paste("PDP/ICE Plot of Sale Price for", lrn_list[[i]], "on Feature", feature))
    }
  } else{
    break
  }
  list(pdp_plots2, ice_plots, pdp_ice_plots)
} 

# still need to add...
## 1. if conditions for imler_global2
## 2. plots for imler_global2
## 3. titles for imler_local plots


#check = trainer(burn, boof, aye)

# lay = predictor(check, home_price, aye[[2]])

# 
# df_raw = loader()
# df = combiner(df_raw, 0.2, 0.1)
# task = tasker(df, "house", target = "sale_price")
# learners = learn_gener(c("regr.rpart", "regr.ranger"))
# datasets = splitter(task, 0.8, 2)
# trained_learners = trainer(learners, task, datasets)
# resample = resampler("cv", 4, 2)
# benchmarker(task, learners, resample, c("regr.mse", "regr.mape"))
# preds = predictor(trained_learners, task, datasets[[2]])
# data_clean = fixer(task, "sale_price") # correct
# imler_global(trained_learners, data_clean[[1]], data_clean[[2]]) # correct
# imler_local(trained_learners, data_clean[[1]], data_clean[[2]], 93)



# # combine and format for mlr3 
# combiner = function(x) {
#   z = bind_rows(x)  %>% 
#     clean_names() %>% # clean column names
#     mutate_if(is.character, as.factor) %>%  # mutate character columns to factors 
#     filter(id <= 1460) 
#   task = TaskRegr$new(id = "houses", backend = z, target = "sale_price")
#   task
# }

# # combine and format the data
# combiner = function(v, perc_miss) {
#   x = bind_rows(v)  %>%
#     clean_names() %>% # clean column names
#     filter(id <= 1460) %>%
#     mutate_if(is.character, as.factor) # mutate character columns to factors
#   y = map_df(x, counter) %>%
#     pivot_longer(-id, names_to = "values", values_to = "count") %>%
#     select(-id) %>%
#     filter(count < perc_miss) %>%
#     pull(values)
#   z = x %>%
#     select(one_of(y)) %>%
#     drop_na() %>%
#     select(-utilities) %>%  # need to drop because its a factor with only 1 level
#     mutate_if(is.factor, lumper)
#   z
# }